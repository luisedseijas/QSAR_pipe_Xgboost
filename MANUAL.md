# QSAR Pipeline User Manual

## Overview
This pipeline allows you to build QSAR models to predict pIC50 values for chemical compounds using XGBoost. It is designed to be easy to use via a terminal interface that handles the complex steps for you.

## üñ•Ô∏è The Interface
The main way to use this tool is through the `pipeline_interface.py` script.

### Starting the Interface
Open your terminal in the project folder and run:
```bash
./pipeline_interface.py
```

### The Menu System
### The Menu System
The interface shows you a detailed status table with 4 columns:

| Column | Meaning |
|:---|:---|
| **Step** | The name of the process step. |
| **Description** | A brief explanation of what the step does. |
| **Input / Output** | üì• **Input**: Files required to start the step.<br>üì§ **Output**: Files generated by the step. |
| **Status** | **UNKNOWN**: Checking state.<br>**MISSING INPUT**: Critical file not found (Red).<br>**READY TO START**: Prerequisites met (Yellow/Cyan).<br>**COMPLETED**: Step finished successfully (Green). |


## üìù Detailed Steps

### Step 1: Dataset Optimization (`dataset_optimizer.py`)
- **What it does**: Reads `data/raw/all_descriptor_results_1751.xlsx`, removes outliers, selects the best features, and creates `data/processed/dataset_molecular_optimizado.xlsx`.
- **When to run**: When you have new raw training data or want to reset the dataset.

### Step 2: Model Training (`xgboost_optimizer.py`)
- **Action**: Trains the XGBoost model using the optimized dataset.
- **Grid Search Configuration**: 
    - When selecting Step 2, you will be prompted to choose a Grid Search mode:
        1. **Default (Exhaustive)**: Uses a large hyperparameter grid. Recommended for final models (Time Consuming).
        2. **Fast (Verification)**: Uses a minimal grid. Recommended for quick testing or debugging.
        3. **Custom (File)**: Loads parameters from a file named `grid_config.json` which **must be located in the project root directory**.
           - **Example `grid_config.json`**:
             ```json
             {
                 "n_estimators": [100, 200],
                 "max_depth": [3, 5],
                 "learning_rate": [0.05, 0.1]
             }
             ```
- **Process**:
  1. Loads optimized data.
  2. It searches for the best "hyperparameters" (settings) to make the model accurate.
- **Note**: This can take a few minutes depending on the settings.

### Output Files
All results are saved in the `results/` directory:

- **`results/model_metadata/`**:
  - `modelo_xgboost_YYYYMMDD_HHMMSS.json`: Trained model file.
  - `metadatos_modelo_YYYYMMDD_HHMMSS.json`: Metrics and parameters.
  
- **`results/plots/`**:
  - **Step 1**:
      - `step_1_1_distribucion_original_ic50.png`
      - `step_1_2_distribucion_log_ic50.png`
      - `step_1_3_comparacion_distribuciones.png`
      - `step_1_5_matriz_correlacion.png`
  - **Step 2**:
      - `step_2_pred_vs_real_train.png`
      - `step_2_pred_vs_real_test.png` (All test points)
      - `step_2_pred_vs_real_test_filtered.png` (Filtered by applicability domain)
      - `step_2_williams_plot_mahalanobis_train.png`
  - **Step 3**:
      - `step_3_distribucion_predicciones_*.png`
      - `step_3_dominio_aplicabilidad_pca_*.png`

- **`results/predictions/`**:
  - `predictions_YYYYMMDD_HHMMSS.xlsx`: Excel file with predictions.

### Step 3: Prediction (`molecular_predictor.py`)
- **What it does**: Reads `data/raw/new_compounds.xlsx` and predicts pIC50 values using the *latest* trained model.
- **Applicability Domain**: It checks if new compounds are "similar enough" to the training data. If a compound is too different (High Mahalanobis Distance), it is marked as "Out of Domain".
- **Output**: Saves an Excel file with predictions to `results/predictions/` and images of the top candidates to `results/plots/`.

## ‚ùì Troubleshooting

### "File Not Found" Errors
- Check `data/raw/`. You must have:
    - `all_descriptor_results_1751.xlsx` (for training phase)
    - `new_compounds.xlsx` (for prediction phase)

### Environment Issues
- The script uses the python environment it is run in.
- Best practice is to create a fresh conda environment:
  ```bash
  conda create -n qsar_env python=3.10
  conda activate qsar_env
  pip install -e .
  ```

### Dependencies
- Most dependencies (like `pandas`, `xgboost`, `rich`, `tqdm`) are automatically installed when you run `pip install -e .`.
